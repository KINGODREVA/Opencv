# annyoing_face

```python
import cv2
import numpy as np
import dlib
from imutils import face_uitils, resize

# 과일 이미지 크기 조절
orange_img = cv2.imread('orange.jpg')
orange_img = cv2.resize(orange_img, dsize=(512, 512))

# 데이터 셋을 가져오기
dectector = dlib.get_front_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# 
# cap = cv2.VideoCapture(0)

# 동영상으로 하기
cap = cv2.VideoCapture('01.mp4')

while cap.isOpened():
    ret, img = cap.read()

    if not ret:
        break
        
    # 얼굴 인식
    faces = dectector(img)
    result = orange_img.copy()
    
    if len(faces) > 0:
        face = faces[0]
        # 얼굴의 왼쪽, 위쪽, 오른쪽, 아래쪽의 좌표 저장하기
        x1, y1, x2, y2 = face.left(), face.top(), face.right(), face.bottom()
        # 얼굴만 복사하기
        face_img = img[y1: y2, x1:x2].copy()

        shape = predictor(img, face)
        shape = face_utils.shape_to_np(shape)

        for p in shape:
            cv2.circle(face_img, center=(p[0]-x1,p[1]-y1), radius=2, color=255, thickness=1)
            
    # 눈 - x축 = 36~39, y축은 37~41
    le_x1 = shape[36, 0]
    le_y1 = shape[37, 1]
    le_x2 = shape[39, 0]
    le_y2 = shape[41, 1]
    
    # 마진을 주기
    le_margin = int((le_x2 - le_x1) * 0.18)
    
    # x축 36~39 , y축 37~41
    re_x1 = shape[42, 0]
    re_y1 = shape[43, 1]
    re_x2 = shape[45, 0]
    re_y2 = shape[47, 1]
    
    # 마진을 주기
    re_margin = int((re_x2 - re_x1) * 0.18)
    
    
    # crop(사진 오리기)
    left_eye_img = img[le_y1-le_margin:le_y2+le_margin, le_x1-le_margin:le_x2+le_margin].copy()
    right_eye_img = img[re_y1-re_margin:re_y2+re_margin, re_x1-re_margin:re_x2+re_margin].copy()
    
    left_eye_img = resize(left_eye_img, width=100) # 가로 100 사이즈 resize
    right_eye_img = resize(right_eye_img, width=100)
    
   # 합성(푸아썸블랜딩)
   result = cv2.seamlessClone( # 푸아썸블랜딩 opencv의 seamlessClone 티가 안나게 합성해준다.
        left_eye_img, # 왼쪽눈 합성
        result, # result 에다가 합성
        np.full(left_eye_img.shape[:2], 255, left_eye_img.dtype),
        (150, 200),
        cv2.MIXED_CLONE # 옵션으로 MIXED_CLONE 하면 지가 알아서 합성해준다.
    )
    result = cv2.seamlessClone(
        right_eye_img, # 오른쪽 눈 합성
        result,
        np.full(right_eye_img.shape[:2], 255, right_eye_img.dtype),
        (300, 200),
        cv2.MIXED_CLONE
    )
    # 여기까지 하면 양쪽눈을 다 맞춘다.

    # mouth
    mouth_x1 = shape[48, 0]
    mouth_y1 = shape[50, 1]
    mouth_x2 = shape[54, 0]
    mouth_y2 = shape[57, 1]
    mouth_margin = int((mouth_x2 - mouth_x1) * 0.1)
        
    # 크롭해서 입 이미지 저장한다.
    mouth_img = img[mouth_y1-mouth_margin:mouth_y2+mouth_margin, mouth_x1-mouth_margin:mouth_x2+mouth_margin].copy()

    mouth_img = resize(mouth_img, width=250)

    result = cv2.seamlessClone(
        mouth_img,
        result,
        np.full(mouth_img.shape[:2], 255, mouth_img.dtype),
        (230, 320),
        cv2.MIXED_CLONE
    )

        # cv2.imshow('left', left_eye_img)
        # cv2.imshow('right', right_eye_img)
        # cv2.imshow('mouth', mouth_img)
    cv2.imshow('face', face_img)
    cv2.imshow('result', result)

    # cv2.imshow('img', img)
    if cv2.waitKey(1) == ord('q'):
        break
        
```

# FACE_DETECTOR(영상에 사진합치기)

```python
import cv2, dlib, sys
import numpy as np

scaler = 0.3

# initialize face detector and shape predictor
detector = dlib.get_frontal_face_detector()
predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')

# 1. load video
cap = cv2.VideoCapture('samples/girl.mp4')
# load overlay image
overlay = cv2.imread('samples/ryan_transparent.png', cv2.IMREAD_UNCHANGED)

# overlay function
def overlay_transparent(background_img, img_to_overlay_t, x, y, overlay_size=None):
  try:
    bg_img = background_img.copy()
    # convert 3 channels to 4 channels
    if bg_img.shape[2] == 3:
      bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGR2BGRA)

    if overlay_size is not None:
      img_to_overlay_t = cv2.resize(img_to_overlay_t.copy(), overlay_size)

    b, g, r, a = cv2.split(img_to_overlay_t)

    mask = cv2.medianBlur(a, 5)

    h, w, _ = img_to_overlay_t.shape
    roi = bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)]

    img1_bg = cv2.bitwise_and(roi.copy(), roi.copy(), mask=cv2.bitwise_not(mask))
    img2_fg = cv2.bitwise_and(img_to_overlay_t, img_to_overlay_t, mask=mask)

    bg_img[int(y-h/2):int(y+h/2), int(x-w/2):int(x+w/2)] = cv2.add(img1_bg, img2_fg)

    # convert 4 channels to 4 channels
    bg_img = cv2.cvtColor(bg_img, cv2.COLOR_BGRA2BGR)

    return bg_img
  except Exception:return background_img

face_roi = []
face_sizes = []

# loop
# 2. 비디오를 계속 프레임 단위로 읽어야하므로
while True:
  # read frame buffer from video
  ret, img = cap.read()
  # 프레임이 없으면 종료 
  if not ret:
    break

  # resize frame
  img = cv2.resize(img, (int(img.shape[1] * scaler), int(img.shape[0] * scaler)))
  ori = img.copy()

  # find faces
  if len(face_roi) == 0:
    faces = detector(img, 1)
  else:
    roi_img = img[face_roi[0]:face_roi[1], face_roi[2]:face_roi[3]]
    # cv2.imshow('roi', roi_img)
    faces = detector(roi_img)

  # no faces
  if len(faces) == 0:
    print('no faces!')

  # find facial landmarks
  for face in faces:
    if len(face_roi) == 0:
      dlib_shape = predictor(img, face)
      shape_2d = np.array([[p.x, p.y] for p in dlib_shape.parts()])
    else:
      dlib_shape = predictor(roi_img, face)
      shape_2d = np.array([[p.x + face_roi[2], p.y + face_roi[0]] for p in dlib_shape.parts()])

    for s in shape_2d:
      cv2.circle(img, center=tuple(s), radius=1, color=(255, 255, 255), thickness=2, lineType=cv2.LINE_AA)

    # compute face center
    #center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int)
    center_x, center_y = np.mean(shape_2d, axis=0).astype(np.int32)

    # compute face boundaries
    min_coords = np.min(shape_2d, axis=0)
    max_coords = np.max(shape_2d, axis=0)

    # draw min, max coords
    cv2.circle(img, center=tuple(min_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)
    cv2.circle(img, center=tuple(max_coords), radius=1, color=(255, 0, 0), thickness=2, lineType=cv2.LINE_AA)

    # compute face size
    face_size = max(max_coords - min_coords)
    face_sizes.append(face_size)
    if len(face_sizes) > 10:
      del face_sizes[0]
    mean_face_size = int(np.mean(face_sizes) * 1.8)

    # compute face roi
    face_roi = np.array([int(min_coords[1] - face_size / 2), int(max_coords[1] + face_size / 2), int(min_coords[0] - face_size / 2), int(max_coords[0] + face_size / 2)])
    face_roi = np.clip(face_roi, 0, 10000)

    # draw overlay on face
    result = overlay_transparent(ori, overlay, center_x + 8, center_y - 25, overlay_size=(mean_face_size, mean_face_size))

  # visualize
  cv2.imshow('original', ori)
  cv2.imshow('facial landmarks', img)
  cv2.imshow('result', result)
  if cv2.waitKey(1) == ord('q'):
    sys.exit(1)
    
```

# 강아지 얼굴 인식

```python
import cv2, dlib, os
import numpy as np
import matplotlib.pyplot as plt
from imutils import face_utils

detector = dlib.cnn_face_detection_model_v1('dogHeadDetector.dat')
predictor = dlib.shape_predictor('landmarkDetector.dat')

# 이미지 불러오기
img_path = 'img/18.jpg'
filename, ext = os.path.splitext(os.path.basename(img_path))
img = cv2.imread(img_path)
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
# img = cv2.resize(img, dsize=None, fx=0.5, fy=0.5)

# plt.figure(figsize=(16, 16))
# plt.imshow(img)
# plt.show()

# 얼굴 인식
dets = detector(img, upsample_num_times=1)

print(dets)

img_result = img.copy()

for i, d in enumerate(dets):
    print("Detection {}: Left: {} Top: {} Right: {} Bottom: {} Confidence: {}".format(i, d.rect.left(), d.rect.top(), d.rect.right(), d.rect.bottom(), d.confidence))

    x1, y1 = d.rect.left(), d.rect.top()
    x2, y2 = d.rect.right(), d.rect.bottom()

    cv2.rectangle(img_result, pt1=(x1, y1), pt2=(x2, y2), thickness=2, color=(255,0,0), lineType=cv2.LINE_AA)
    
shapes = []

for i, d in enumerate(dets):
    shape = predictor(img, d.rect)
    shape = face_utils.shape_to_np(shape)
    
    for i, p in enumerate(shape):
        shapes.append(shape)
        cv2.circle(img_result, center=tuple(p), radius=3, color=(0,0,255), thickness=-1, lineType=cv2.LINE_AA)
        cv2.putText(img_result, str(i), tuple(p), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1, cv2.LINE_AA)

img_out = cv2.cvtColor(img_result, cv2.COLOR_RGB2BGR)
cv2.imwrite('img/%s_out%s' % (filename, ext), img_out)

plt.figure(figsize=(16, 16))
plt.imshow(img_result)
plt.show()
```
